//spark2-shell

def clean(pair : String) : org.apache.spark.sql.DataFrame = {
  val sqlContext = new org.apache.spark.sql.SQLContext(sc)
  val data = sqlContext.read.format("csv").option("header", "true").option("inferSchema", "true").load("/user/pjv253/valhalla/" + pair + ".csv")
  val dropped = data.drop("buy_or_sell").drop("limit_or_market").drop(data.schema.fields(5).name).na.drop
  val convert_millis = udf((v: Double) => (v * 1000).toLong )
  val converted = dropped.withColumnRenamed("timestamp", "timestamp_seconds").withColumn("timestamp", convert_millis($"timestamp_seconds")).drop("timestamp_seconds")
  val make_bucket = () => udf((timestamp: Long) => timestamp / (1000L * 60) )
  val partitioned = converted.withColumn("bucket", make_bucket()($"timestamp"))
  val weighted_price = udf[Double, Seq[Double], Seq[Double]]((prices,quantities) => {
    val data = prices.zip(quantities)
    var wsum = 0.0
    var qsum = 0.0
    for(i <- 0 until data.size) {
      var px = data(i)._1
      var qx = data(i)._2
      if (qx == 0.0) {
         qx = 0.001
      }
      wsum += px*qx
      qsum += qx
    }
    val weighted_px = wsum / qsum
    weighted_px
  })
  var bucketed = partitioned.groupBy("bucket").agg(collect_list(col("price")) as "price", collect_list(col("quantity")) as "quantity").withColumn("price", weighted_price(col("price"), col("quantity"))).drop("quantity")
  val make_partition = () => udf((timestamp: Long) => timestamp / (60*24*3) )
  bucketed = bucketed.withColumn("partition", make_partition()($"bucket"))
  val sorted = bucketed.sort("bucket")

  val lag5b = org.apache.spark.sql.expressions.Window.partitionBy("partition").orderBy("bucket").rowsBetween(-5,0)
  val lag5f = org.apache.spark.sql.expressions.Window.partitionBy("partition").orderBy("bucket").rowsBetween(0,5)
  val lag60b = org.apache.spark.sql.expressions.Window.partitionBy("partition").orderBy("bucket").rowsBetween(-60,0)
  val lag60f = org.apache.spark.sql.expressions.Window.partitionBy("partition").orderBy("bucket").rowsBetween(0,60)
  var ma = sorted.withColumn("ma5b", avg(sorted("price")).over(lag5b))
  ma = ma.withColumn("ma5f", avg(ma("price")).over(lag5f))
  ma = ma.withColumn("dpx5", ma("price") - ma("ma5b"))
  ma = ma.withColumn("fdp5", ma("ma5f") - ma("price"))

  ma = ma.withColumn("ma60b", avg(sorted("price")).over(lag60b))
  ma = ma.withColumn("ma60f", avg(ma("price")).over(lag60f))
  ma = ma.withColumn("dpx60", ma("price") - ma("ma60b"))
  ma = ma.withColumn("fdp60", ma("ma60f") - ma("price"))

  val file = "/user/jr4716/" + pair + "_5_60_FINAL.csv"
  val path = new org.apache.hadoop.fs.Path(file)
  val fs=org.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration)
  fs.delete(path, true)
  ma.write.format("csv").option("header", "true").save(file)
  val fileList = fs.listFiles(path, true)
  val permission = new org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsAction.ALL,org.apache.hadoop.fs.permission.FsAction.READ_EXECUTE,org.apache.hadoop.fs.permission.FsAction.READ_EXECUTE)
  while (fileList.hasNext()) {
    fs.setPermission(fileList.next().getPath(),permission)
 }
  return ma
}

val currencies = Array("XXBTZUSD", "XLTCZUSD", "XXRPZUSD", "XETHZUSD")
currencies.foreach(clean)
