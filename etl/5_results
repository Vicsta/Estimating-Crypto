//spark2-shell

import org.apache.spark.ml.regression.{LinearRegression,LinearRegressionModel}
import org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}
import org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}
import org.apache.spark.ml.regression.{DecisionTreeRegressionModel,DecisionTreeRegressor}
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.feature.Normalizer
import org.apache.spark.ml.feature.StandardScaler
import org.apache.spark.sql._
import scala.collection.mutable.ArrayBuffer
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs._

implicit class Crossable[X](xs: Traversable[X]) {
  def cross[Y](ys: Traversable[Y]) = for { x <- xs; y <- ys } yield (x, y)
}

def featIndex(n : Int) : Array[Int] = {
  var b = ArrayBuffer.fill[Int](n)(0)
  for(i <- 0 until n) {
    b(i) = 3 + 3*(i)
  }
  b.toArray
}

def chmod(file: String) = {
  val path = new org.apache.hadoop.fs.Path(file)
  val fs=org.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration)
  val fileList = fs.listFiles(path, true)
  val permission = new org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsAction.ALL,org.apache.hadoop.fs.permission.FsAction.READ_EXECUTE,org.apache.hadoop.fs.permission.FsAction.READ_EXECUTE)
  while (fileList.hasNext()) {
    fs.setPermission(fileList.next().getPath(),permission)
  }
}

def fitLinear(data: org.apache.spark.sql.DataFrame) = {
    val lr = new LinearRegression().setMaxIter(200).setFeaturesCol("features").setFitIntercept(false)
    val model = lr.fit(data)
    model
}

def fitScaled(data: org.apache.spark.sql.DataFrame) = {
    val scale = new StandardScaler()
    .setInputCol("features")
    .setOutputCol("scaledFeatures")
    .setWithStd(true)
    .setWithMean(false)
    val smodel = scale.fit(data)
    val sdata = smodel.transform(data)
    val lr = new LinearRegression().setMaxIter(200).setFeaturesCol("scaledFeatures").setFitIntercept(false)
    val model = lr.fit(sdata)
    model
}

def fitRandomForest(data: org.apache.spark.sql.DataFrame) = {
  val lr = new RandomForestRegressor().setFeaturesCol("features")
  val model = lr.fit(data)
  model
}

def fitGradientBoostedTree(data: org.apache.spark.sql.DataFrame) = {
  val lr = new GBTRegressor().setFeaturesCol("features")
  val model = lr.fit(data)
  model
}

def fitDecisionTree(data: org.apache.spark.sql.DataFrame) = {
  val lr = new DecisionTreeRegressor().setFeaturesCol("features")
  val model = lr.fit(data)
  model
}

def featIndex(n : Int) : Array[Int] = {
  var b = ArrayBuffer.fill[Int](n)(0)
  for(i <- 0 until n) {
    b(i) = 3 + 3*(i)
  }
  b.toArray
}

def train(pair: String, algo: String) =  {
  val models = Map(
                   "linear" -> fitLinear _,
                   "linear_scaled" -> fitScaled _,
                   "random_forest" -> fitRandomForest _,
                   "gradient_boosted_tree" -> fitGradientBoostedTree _,
                   "decision_tree" -> fitDecisionTree _)

  val sqlContext = new org.apache.spark.sql.SQLContext(sc)
  val dataSet = sqlContext.read.format("csv").option("header", "true").option("inferSchema", "true").load("/user/pjv253/valhalla/MERGED_FINAL.csv")
  val features = featIndex(4)
  val trainPct : Double = 0.50
  val seed : Long = 12345L
  val Array(training, test) = dataSet.randomSplit(Array(trainPct, (1.0 - trainPct)), seed)
  val ri = training.columns.indexOf(pair + "_fwdDelta")

  def rdata(row : org.apache.spark.sql.Row, fi:Array[Int]) = {
     (row(ri).asInstanceOf[Double], Vectors.dense(fi.map(i => row(i).asInstanceOf[Double])))
  }

  val fitdata = training.map(s => rdata(s, features)).toDF("label", "features")
  val model = models(algo)(fitdata)
  val file = "/user/pjv253/valhalla/models/"+pair +"_" + algo
  val path = new org.apache.hadoop.fs.Path(file)
  val fs=org.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration)
  fs.delete(path, true)
  model.save(file)
  val fileList = fs.listFiles(path, true)
  val permission = new org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsAction.ALL,org.apache.hadoop.fs.permission.FsAction.READ_EXECUTE,org.apache.hadoop.fs.permission.FsAction.READ_EXECUTE)
  while (fileList.hasNext()) {
    fs.setPermission(fileList.next().getPath(),permission)
  }
}

def results(pair: String, algo: String) =  {
  val models = Map(
                   "linear" -> LinearRegressionModel,
                   "linear_scaled" -> LinearRegressionModel,
                   "random_forest" -> RandomForestRegressionModel,
                   "gradient_boosted_tree" -> GBTRegressionModel,
                   "decision_tree" -> DecisionTreeRegressionModel
                  )

  val sqlContext = new org.apache.spark.sql.SQLContext(sc)
  val dataSet = sqlContext.read.format("csv").option("header", "true").option("inferSchema", "true").load("/user/pjv253/valhalla/MERGED_FINAL.csv")
  val features = featIndex(4)
  val file = "/user/pjv253/valhalla/models/"+pair +"_" + algo
  val model = models(algo).load(file)
  val price_index = dataSet.columns.indexOf(pair + "_price")
  val dpx_index = dataSet.columns.indexOf(pair + "_dpx")
  val bucket_index = dataSet.columns.indexOf("bucket")

  def rdata(row : org.apache.spark.sql.Row, fi:Array[Int]) = {
     (row(bucket_index).asInstanceOf[Int], row(price_index).asInstanceOf[Double],row(dpx_index).asInstanceOf[Double], Vectors.dense(fi.map(i => row(i).asInstanceOf[Double])))
  }

  var data = dataSet.map(s => rdata(s, features)).toDF("bucket", "price","dpx", "features")
  if(algo.equals("linear_scaled")) {
    val scale = new StandardScaler()
    .setInputCol("features")
    .setOutputCol("scaledFeatures")
    .setWithStd(true)
    .setWithMean(false)
    val smodel = scale.fit(data)
    data = smodel.transform(data)
  }
  val prediction = model.transform(data).select("bucket", "price", "prediction")
  val apply_prediction = () => udf[Double, Double, Double]((price:Double, prediction:Double) => price +  prediction)
  val apply_timestamp = () => udf[Double, Double]((bucket:Double) => bucket * 60 * 1000)
  var results = prediction.withColumn("predicted", apply_prediction()($"price", $"prediction")).drop("prediction")
  results = results.withColumn("timestamp", apply_timestamp()($"bucket")).drop("bucket")
  val w = org.apache.spark.sql.expressions.Window.orderBy("timestamp").rowsBetween(1, 11)
  results = results.withColumn("price", avg("price").over(w))
  val apply_hours = () => udf[Double, Double]((timestamp:Double) => ((timestamp/(1000 * 60 * 60)).toLong * (1000 * 60 * 60)))
  results = results.withColumn("timestamp", apply_hours()($"timestamp"))
  results = results.groupBy("timestamp").agg(avg("price").as("price"), avg("predicted").as("predicted"))
  results = results.sort("timestamp").na.drop
  val csvFile = "/user/pjv253/valhalla/results/"+pair +"_" + algo + ".csv"
  results.coalesce(1).write.format("csv").option("header", "true").mode("overwrite").save(csvFile)
}

def merge(pair: String, algo: String) =  {
  val csvFile = "/user/pjv253/valhalla/results/"+pair +"_" + algo + ".csv"
  val mergedCsvFile = "/user/pjv253/valhalla/results/"+pair +"_" + algo + "_MERGED.csv"
  val fs=org.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration)
  fs.delete(new Path(mergedCsvFile), true)
  FileUtil.copyMerge(fs, new Path(csvFile), fs, new Path(mergedCsvFile), true, sc.hadoopConfiguration, null)
}

val currencies = List("XXBTZUSD", "XLTCZUSD", "XXRPZUSD", "XETHZUSD")
val models = List("linear", "linear_scaled", "random_forest", "gradient_boosted_tree", "decision_tree")

(currencies cross models).foreach(x => train(x._1, x._2))
(currencies cross models).foreach(x => results(x._1, x._2))
(currencies cross models).foreach(x => merge(x._1, x._2))
