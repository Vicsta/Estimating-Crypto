

import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.feature.Normalizer
import org.apache.spark.ml.feature.StandardScaler
import org.apache.spark.sql._
import scala.collection.mutable.ArrayBuffer

implicit class Crossable[X](xs: Traversable[X]) {
  def cross[Y](ys: Traversable[Y]) = for { x <- xs; y <- ys } yield (x, y)
}

def fitLinear(data: org.apache.spark.sql.DataFrame) = {
    val lr = new LinearRegression().setMaxIter(200).setFeaturesCol("features")
    val model = lr.fit(data)
    val prediction = model.transform(data).select("features", "prediction", "label")
    model
}

def fitScaled(data: org.apache.spark.sql.DataFrame) = {
    val scale = new StandardScaler()
    .setInputCol("features")
    .setOutputCol("scaledFeatures")
    .setWithStd(true)
    .setWithMean(false)
    val smodel = scale.fit(data)
    val sdata = smodel.transform(data)
    val lr = new LinearRegression().setMaxIter(200).setFeaturesCol("scaledFeatures")
    val model = lr.fit(sdata)
    model
}

def featIndex(n : Int) : Array[Int] = {
  var b = ArrayBuffer.fill[Int](n)(0)
  for(i <- 0 until n) {
    b(i) = 3 + 3*(i)
  }
  b.toArray
}

def train(pair: String, algo: String) =  {
  val models = Map(
                   "linear" -> fitLinear _,
                   "linear_scaled" -> fitScaled _)

  val sqlContext = new org.apache.spark.sql.SQLContext(sc)
  val dataSet = sqlContext.read.format("csv").option("header", "true").option("inferSchema", "true").load("/user/pjv253/valhalla/MERGED_FINAL.csv")
  val features = featIndex(4)
  val ri = dataSet.columns.indexOf(pair + "_fwdDelta")

  def rdata(row : org.apache.spark.sql.Row, fi:Array[Int]) = {
     (row(ri).asInstanceOf[Double], Vectors.dense(fi.map(i => row(i).asInstanceOf[Double])))
  }

  val fitdata = dataSet.map(s => rdata(s, features)).toDF("label", "features")
  val model = models(algo)(fitdata)
  val file = "/user/pjv253/valhalla/models/"+pair +"_" + algo
  val path = new org.apache.hadoop.fs.Path(file)
  val fs=org.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration)
  model.write.overwrite().save(file)
  val fileList = fs.listFiles(path, true)
  val permission = new org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsAction.ALL,org.apache.hadoop.fs.permission.FsAction.READ_EXECUTE,org.apache.hadoop.fs.permission.FsAction.READ_EXECUTE)
  while (fileList.hasNext()) {
    fs.setPermission(fileList.next().getPath(),permission)
  }
}

val currencies = List("XXBTZUSD", "XLTCZUSD", "XXRPZUSD", "XETHZUSD")
val models = List("linear", "linear_scaled")

(currencies cross models).foreach(x => train(x._1, x._2))
