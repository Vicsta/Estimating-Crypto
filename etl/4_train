import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.feature.Normalizer
import org.apache.spark.ml.feature.StandardScaler
import org.apache.spark.sql._
import scala.collection.mutable.ArrayBuffer

def regdf(fileName : String) : org.apache.spark.sql.DataFrame = {
    sc.textFile(fileName).map(_.split(",")).map(s => (s(2).toDouble, Vectors.dense(s(0).toDouble, s(1).toDouble))).toDF("label", "features")
}

def printSummary(model: org.apache.spark.ml.regression.LinearRegressionModel, coeff: org.apache.spark.ml.linalg.Vector) = {
    printf("R-squared = %f\n", model.summary.r2)
    printf("MSE       = %f\n", model.summary.meanSquaredError)
    for(i <- 0 until coeff.size) {
       printf("(feature_%d,tvalue) = (%f, %f)\n", i, coeff(i), model.summary.tValues(i))
    }
}

def fitLinear(data: org.apache.spark.sql.DataFrame) = {
    val lr = new LinearRegression().setMaxIter(200).setFeaturesCol("features")
    val model = lr.fit(data)
    val prediction = model.transform(data).select("features", "prediction", "label")
    model
}

def fitScaled(data: org.apache.spark.sql.DataFrame) = {

    val scale = new StandardScaler()
    .setInputCol("features")
    .setOutputCol("scaledFeatures")
    .setWithStd(true)
    .setWithMean(false)

    val smodel = scale.fit(data)
    val sdata = smodel.transform(data)

    val lr = new LinearRegression().setMaxIter(200).setFeaturesCol("scaledFeatures")
    val model = lr.fit(sdata)
    val pred = model.transform(sdata).select("features", "scaledFeatures", "prediction")

    val coeff = model.coefficients
    val std = smodel.std
    val unscaled : scala.collection.mutable.ArrayBuffer[Double] = ArrayBuffer.fill[Double](coeff.size)(0.0)
    for(i <- 0 until coeff.size) {
       unscaled(i) = coeff(i) / std(i)
    }
    val uv = org.apache.spark.ml.linalg.Vectors.dense(unscaled.toArray)

    (model, uv)
}


val fileName : String = "hdfs:///user/jr4716/gen.csv"
val regressionSet = regdf(fileName)

val fitModel = fitLinear(regressionSet)
printSummary(fitModel, fitModel.coefficients)

val scaled = fitScaled(regressionSet)
printSummary(scaled._1, scaled._2)

val sqlContext = new org.apache.spark.sql.SQLContext(sc)
var data = sqlContext.read.format("csv").option("header", "true").option("inferSchema", "true").load("/user/pjv253/valhalla/MERGED_FINAL.csv")
def regdf(pair : String) : org.apache.spark.sql.DataFrame = {
    sc.textFile(fileName).map(_.split(",")).map(s => (s(2).toDouble, Vectors.dense(s(0).toDouble, s(1).toDouble))).toDF("label", "features")
}
def train(pair : String, model: String) :  org.apache.spark.ml.Model = {

  return null;
}

implicit class Crossable[X](xs: Traversable[X]) {
  def cross[Y](ys: Traversable[Y]) = for { x <- xs; y <- ys } yield (x, y)
}

val models = Map("linear" -> fitLinear _)
val currencies = List("XXBTZUSD", "XLTCZUSD", "XXRPZUSD", "XETHZUSD")
(models.keys cross currencies).foreach(println)
val m = train("XXBTZUSD", "linear")

val fileName : String = "hdfs:///user/pjv253/valhalla/gen.csv"
