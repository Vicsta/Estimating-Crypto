#Run the spark shell with csv support.
#spark-shell --packages com.databricks:spark-csv_2.11:1.2.0

def clean(pair : String) : org.apache.spark.sql.DataFrame = {
  val pair = "XLTCZUSD"
  val data = sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("/user/pjv253/valhalla/" + pair + ".csv")
  val dropped = data.na.drop.drop("")
  val convert_millis = udf((v: Double) => (v * 1000).toLong )
  val converted = dropped.withColumnRenamed("timestamp", "timestamp_seconds").withColumn("timestamp", convert_millis($"timestamp_seconds")).drop("timestamp_seconds")
  val sorted = converted.sort("timestamp")
  var windowed = sorted
  val convert_window = (w:Long) => udf((timestamp: Long) => timestamp / w )
  val partitioned = windowed.withColumn("partition", convert_window(1000L * 60*60*24*3)($"timestamp"))
  val w = org.apache.spark.sql.expressions.Window.partitionBy($"partition").orderBy("timestamp")
  val emaF = udf((price_t: Double, ema_p: Double) => {
    if(ema_p.isNaN) {
      price_t
    } else {
      (price_t * 0.4) + (ema_p.get * 0.6)
    }
  })
  val fill = partitioned.withColumn("ema", $"price")
  val ema = fill.withColumn("ema", emaF($"price", lag($"ema", 1).over(w))).na.drop

  val file = "/user/pjv253/valhalla/" + pair + "_CLEAN.csv"
  val path = new org.apache.hadoop.fs.Path(file)
  val fs=org.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration)
  fs.delete(path, true)
  ema.coalesce(1).write.format("com.databricks.spark.csv").option("header", "true").save(file)
  val fileList = fs.listFiles(path, true)
  val permission = new org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsAction.ALL,org.apache.hadoop.fs.permission.FsAction.READ_EXECUTE,org.apache.hadoop.fs.permission.FsAction.READ_EXECUTE)
  while (fileList.hasNext()) {
    fs.setPermission(fileList.next().getPath(),permission)
  }

  return sorted
}

val currencies = Array("XETHZUSD", "XLTCZUSD", "XXBTZUSD", "XXRPZUSD")
currencies.foreach(clean)
