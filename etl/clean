//spark2-shell

def clean(pair : String) : org.apache.spark.sql.DataFrame = {
  val sqlContext = new org.apache.spark.sql.SQLContext(sc)
  val data = sqlContext.read.format("csv").option("header", "true").option("inferSchema", "true").load("/user/pjv253/valhalla/" + pair + ".csv")
  val dropped = data.drop("buy_or_sell").drop("limit_or_market").drop("quantity").drop(data.schema.fields(5).name).na.drop
  val convert_millis = udf((v: Double) => (v * 1000).toLong )
  val converted = dropped.withColumnRenamed("timestamp", "timestamp_seconds").withColumn("timestamp", convert_millis($"timestamp_seconds")).drop("timestamp_seconds")
  val sorted = converted.sort("timestamp")
  val january_epoch = 1514764800000L
  val filtered = sorted.filter(converted("timestamp") >= january_epoch)
  val convert_window = (w:Long) => udf((timestamp: Long) => timestamp / w )
  val partitioned = filtered.withColumn("bucket", convert_window(1000L * 60)($"timestamp"))
  val min_bucket = partitioned.select(min("bucket")).first().getLong(0).toInt
  val max_bucket = partitioned.select(max("bucket")).first().getLong(0).toInt
  val schema = List(org.apache.spark.sql.types.StructField("bucket", org.apache.spark.sql.types.LongType, true))
  val range = sc.parallelize(Range(min_bucket, max_bucket + 1, 1)).map(_.toLong).toDF("bucket")
  val bucketed = range.join(partitioned, range("bucket") === partitioned("bucket"), "left").drop(partitioned("bucket"))
  var fill = bucketed
  fill = fill.withColumn("partition", convert_window(60*24*10)($"bucket"))
  val forward = org.apache.spark.sql.expressions.Window.partitionBy("partition").orderBy("bucket","timestamp").rowsBetween(-14400, 0)
  val filled_column_last = last(fill("price"), true).over(forward)
  fill = fill.withColumn("price_filled", filled_column_last )
  fill = fill.drop("price").withColumnRenamed("price_filled", "price")
  fill = fill.sort("bucket", "timestamp")
  fill = fill.drop("timestamp").drop("partition")

  val file = "/user/pjv253/valhalla/" + pair + "_CLEAN.csv"
  val path = new org.apache.hadoop.fs.Path(file)
  val fs=org.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration)
  fs.delete(path, true)
  fill.write.format("csv").option("header", "true").save(file)
  val fileList = fs.listFiles(path, true)
  val permission = new org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsAction.ALL,org.apache.hadoop.fs.permission.FsAction.READ_EXECUTE,org.apache.hadoop.fs.permission.FsAction.READ_EXECUTE)
  while (fileList.hasNext()) {
    fs.setPermission(fileList.next().getPath(),permission)
  }
  return sorted
}

val currencies = Array("XXBTZUSD", "XLTCZUSD", "XXRPZUSD", "XETHZUSD")
currencies.foreach(clean)
