//spark2-shell

val min_bucket = partitioned.select(min("bucket")).first().getLong(0).toInt
val max_bucket = partitioned.select(max("bucket")).first().getLong(0).toInt
val schema = List(org.apache.spark.sql.types.StructField("bucket", org.apache.spark.sql.types.LongType, true))
val range = sc.parallelize(Range(min_bucket, max_bucket + 1, 1)).map(_.toLong).toDF("bucket")
val bucketed = range.join(partitioned, range("bucket") === partitioned("bucket"), "left").drop(partitioned("bucket"))
var fill = bucketed

val convert_window = (w:Long) => udf((timestamp: Long) => timestamp / w )
fill = fill.withColumn("partition", convert_window(60*24*10)($"bucket"))
val forward = org.apache.spark.sql.expressions.Window.partitionBy("partition").orderBy("bucket","timestamp").rowsBetween(-14400, 0)
val filled_column_last = last(fill("price"), true).over(forward)
fill = fill.withColumn("price_filled", filled_column_last )
fill = fill.drop("price").withColumnRenamed("price_filled", "price")
fill = fill.sort("bucket", "timestamp")
fill = fill.drop("timestamp").drop("partition")
val grouped = fill.groupBy("bucket").agg(collect_list("price"))

def merge(currencies : Array[String]) : org.apache.spark.sql.DataFrame = {
  val sqlContext = new org.apache.spark.sql.SQLContext(sc)
  val dataFrames: Map[String, org.apache.spark.sql.DataFrame] = currencies.map(pair => {
      var data = sqlContext.read.format("csv").option("header", "true").option("inferSchema", "true").load("/user/pjv253/valhalla/" + pair + "_CLEAN.csv")
      val convert_window = (w:Long) => {
        udf((timestamp: Long) => (timestamp / w) * w  )
      }
      data = data.withColumn("timestamp", convert_window(1000L * 60)($"timestamp"))
      data = data.groupBy("timestamp").agg(avg("price"), avg("ma"))
      val fields = data.schema.fields.toList
      fields.foreach((field)=> {
        if(!field.name.equals("timestamp")) {
          data = data.withColumnRenamed(field.name, pair + "_" + field.name)
        }
      })
      (pair, data)
  }).toMap
  val head = dataFrames.head._1
  var merged = dataFrames.head._2
  dataFrames.foreach(pair => {
    val key = pair._1
    val data = pair._2
    if(!key.equals(head)) {
      merged = merged.join(data, merged("timestamp") === data("timestamp"), "fullouter").drop(data("timestamp"))
    }
  })

  var fill = merged
  val convert_window = (w:Long) => udf((timestamp: Long) => timestamp / w )
  fill = fill.withColumn("partition", convert_window(1000L * 60*60*24*3)($"timestamp"))
  val fields = fill.schema.fields.toList
  fields.foreach((field)=> {
    if(!field.name.equals("timestamp")) {
      val forward = org.apache.spark.sql.expressions.Window.partitionBy("partition").orderBy("timestamp").rowsBetween(-1000000000, 0)
      val filled_column_last = last(fill(field.name), true).over(forward)
      fill = fill.withColumn(field.name + "_filled", filled_column_last )
      fill = fill.drop(field.name).withColumnRenamed(field.name + "_filled", field.name)
      val filled_column_first = first(fill(field.name)).over(forward)
    }
  })

  fill = fill.na.drop

  val file = "/user/pjv253/valhalla/merged_CLEAN.csv"
  val path = new org.apache.hadoop.fs.Path(file)
  val fs=org.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration)
  fs.delete(path, true)
  fill.write.format("csv").option("header", "true").save(file)
  val fileList = fs.listFiles(path, true)
  val permission = new org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsAction.ALL,org.apache.hadoop.fs.permission.FsAction.READ_EXECUTE,org.apache.hadoop.fs.permission.FsAction.READ_EXECUTE)
  while (fileList.hasNext()) {
    fs.setPermission(fileList.next().getPath(),permission)
  }
  return merged
}

val currencies = Array("XXBTZUSD", "XLTCZUSD", "XXRPZUSD", "XETHZUSD")
merge(currencies)
